<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
<title>BERT - Gilmore&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta property="og:type" content="article">
<meta property="og:title" content="BERT">
<meta property="og:url" content="http://yoursite.com/2019/11/27/BERT/index.html">
<meta property="og:site_name" content="Gilmore&#39;s Blog">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/11/27/BERT/1.jpg">
<meta property="og:updated_time" content="2019-11-30T17:41:07.600Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT">
<meta name="twitter:image" content="http://yoursite.com/2019/11/27/BERT/1.jpg">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-dune-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/categories/LifeStyle">Lifestyle</a>
            
            <a class="navbar-item " href="/categories/Interviews">Interviews</a>
            
            <a class="navbar-item " href="/categories/Projects">Projects</a>
            
            <a class="navbar-item " href="/categories/Learning">Learning</a>
            
            <a class="navbar-item " href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="Table of Contents">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#Abstract">1&nbsp;&nbsp;<b>Abstract</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#Two-existing-strategies">2&nbsp;&nbsp;<b>Two existing strategies</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#BERT-Unique">3&nbsp;&nbsp;<b>BERT Unique</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#History">4&nbsp;&nbsp;<b>History</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#Unsupervised-Feature-based-Approaches">4.1&nbsp;&nbsp;Unsupervised Feature-based Approaches</a>
                    
                    
                    
                    <a class="navbar-item" href="#Unsupervised-Fine-tuning-Approaches">4.2&nbsp;&nbsp;Unsupervised Fine-tuning Approaches</a>
                    
                    
                    
                    <a class="navbar-item" href="#Transfer-Learning-from-Supervised-Data">4.3&nbsp;&nbsp;Transfer Learning from Supervised Data</a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#BERT">5&nbsp;&nbsp;<b>BERT</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#Model-Architecture">5.1.1&nbsp;&nbsp;Model Architecture</a>
                    
                    
                    
                    <a class="navbar-item" href="#Input-Output-Representations">5.1.2&nbsp;&nbsp;Input/Output Representations</a>
                    
                    
                    
                    <a class="navbar-item" href="#Pre-training-BERT">5.2&nbsp;&nbsp;Pre-training BERT</a>
                    
                    
                    
                    <a class="navbar-item" href="#Fine-tuning-BERT">5.3&nbsp;&nbsp;Fine-tuning BERT</a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/Gilemore">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            BERT
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-11-27T15:59:03.000Z" itemprop="datePublished">Nov 27 2019</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Projects/">Projects</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            5 minutes read (About 822 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p><img src="/2019/11/27/BERT/1.jpg" alt></p>
<a id="more"></a>

<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>BERT: <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformer</p>
<p>Using as “masked language model” <strong><em>(MLM)</em></strong> pre-training objective to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on <strong><em>both left and right context</em></strong> in all layers.</p>
<p>So the pre-trained BERT model can be fine-tuned with just one additional output layer to create the state-of-the-art models for a wide range of tasks, such as question answering (SQuAD 2.0, we are trying to do this without BERT…) and language inference.</p>
<h1 id="Two-existing-strategies"><a href="#Two-existing-strategies" class="headerlink" title="Two existing strategies"></a>Two existing strategies</h1><ul>
<li><strong><em>Feature-based:</em></strong> uses task-specific architectures that include the pre-trained representations as additional features (ELMo)</li>
<li><strong><em>Fine-tuning:</em></strong> introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. (BERT)</li>
</ul>
<h1 id="BERT-Unique"><a href="#BERT-Unique" class="headerlink" title="BERT Unique"></a>BERT Unique</h1><ul>
<li>unidirectional language models for pre-training</li>
<li>shallow concatenation of independently trained left-to-right and right-to-left LMs. (I always thought BERT is the first one to introduce Transformers…..)</li>
</ul>
<h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><h2 id="Unsupervised-Feature-based-Approaches"><a href="#Unsupervised-Feature-based-Approaches" class="headerlink" title="Unsupervised Feature-based Approaches"></a>Unsupervised Feature-based Approaches</h2><p>ELMo generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations.</p>
<h2 id="Unsupervised-Fine-tuning-Approaches"><a href="#Unsupervised-Fine-tuning-Approaches" class="headerlink" title="Unsupervised Fine-tuning Approaches"></a>Unsupervised Fine-tuning Approaches</h2><p>As with the feature-based approaches, the first works in this direction only <strong><em>pre-trained word embedding parameters from unlabeled text.</em></strong></p>
<p>More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. Advantage: <strong><em>few parameters need to be learned from scratch</em></strong>. </p>
<h2 id="Transfer-Learning-from-Supervised-Data"><a href="#Transfer-Learning-from-Supervised-Data" class="headerlink" title="Transfer Learning from Supervised Data"></a>Transfer Learning from Supervised Data</h2><h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>there are two steps: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-trianing tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. </p>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>Multi-layer bidirectional Transformer encoder</p>
<h3 id="Input-Output-Representations"><a href="#Input-Output-Representations" class="headerlink" title="Input/Output Representations"></a>Input/Output Representations</h3><p>can be a single sentence or two sentences packed together: &lt;Question, Answer&gt;.</p>
<p>The first token of every sequence is always a special classification token (<code>[CLS]</code>). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</p>
<p>Sentence pairs are packed together into a single sequence. </p>
<p>We differentiate the sentences in two ways. First, we separate them with a special token <code>([SEP])</code> . Second, we add a learned embedding to every token indicating whether it belongs to sentence <code>A</code> or sentence <code>B</code>. </p>
<p>For a given token, its input representation is constructed by summing the <strong><em>corresponding token</em></strong>, <strong><em>segment (A or B)</em></strong>, and <strong><em>position embeddings</em></strong>. </p>
<h2 id="Pre-training-BERT"><a href="#Pre-training-BERT" class="headerlink" title="Pre-training BERT"></a>Pre-training BERT</h2><p>we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks.</p>
<p><strong><em>Task 1: Masked LM</em></strong></p>
<p>Intuitively, a deep bidirectional model (transformer) is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.</p>
<p>But standard conditional language models can only trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. </p>
<p>In order to train a deep bidirectional representation, we simply mask some precentage (15%) of the input tokens at random and then predict those masked tokens. </p>
<p><strong><em>Task 2: Next Sentence Prediction (NSP)</em></strong></p>
<p>Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. So we pre-train for a <em>binarized next sentence prediction task</em> that can be trivially generated from any monolingual corpus.</p>
<p>That is, when choosing the sentences <code>A</code> and <code>B</code> for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). </p>
<p>BERT transfers all parameters to initialize end-task model parameters.</p>
<h2 id="Fine-tuning-BERT"><a href="#Fine-tuning-BERT" class="headerlink" title="Fine-tuning BERT"></a>Fine-tuning BERT</h2><p>For text pairts, common method is to independently encode text pairs before applying bidirectional cross attention. </p>
<p>BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. </p>
<p>at the input, sentence <code>A</code> and sentence <code>B</code> from pre-training are analogus to:</p>
<ul>
<li>sentence pairs in paraphrasing</li>
<li>hypothesis-premise pairs in entailment (homework 3)</li>
<li>question-passage pairs in question answering (homework 5)</li>
<li>a degenerate text-$\empty$ pair in text classification or sequence tagging.</li>
</ul>
<p>at the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering (generating a detailed output), and the <code>[CLS]</code> representation (this already summarize the input) is fed into an output layer for classification, such as entailment or sentiment analysis. </p>

    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2019/11/27/engagement/">Engagement</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2019/11/26/object-oriented-programming/">object-oriented programming</a>
            
        </span>
    </div>
    
</article>




    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 Haoxue Li&nbsp;
                ONLY SINGLE ONE IN NYC: <a href="https://github.com/Gilemore">Gilemore</a>
      
            </div>
            <div class="column is-hidden-mobile"></div>

            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>